{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Engineer exercises**\n",
    "- Date: 9/16/2020\n",
    "- Purpose: ALS Data Engineer exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary code to structure notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML code to left align markdown tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "javacript code to update ToC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This notebook contains exploratory data analysis and solutions to the Anne Lewis Strategies Hiring Data Engineer Exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**Produce a “people” file with the following schema. Save it as a CSV with a header line to the working directory.**\n",
    "\n",
    "    \n",
    "| Column | Type | Description |\n",
    "| :-- | :-- | :-- |\n",
    "|email | string | Primary email address | \n",
    "|code | string | Source code |\n",
    "|is_unsub | boolean | If primary email address is unsubscribed |\n",
    "|created_dt | datetime | Person creation datetime |\n",
    "|updated_dt | datetime | Person updated datetime |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Use the output of #1 to produce an “acquisition_facts” file with the following schema that aggregates stats about when people in the dataset were acquired. Save it to the working directory.**\n",
    "\n",
    "| Column | Type | Description | \n",
    "| :-- | :-- | :-- |\n",
    "| acquisition_date | date | Calendar date of acquisition | \n",
    "| acquisitions | int | Number of constituents acquired on acquisition date |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Notes\n",
    "This section contains notes on the methodologies used in data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data frame notes\n",
    "\n",
    "The following dataframes are used to reference the respective Constituent data.\n",
    "\n",
    "1. `df_info` : [Constituent Information](https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons.csv)\n",
    "2. `df_email` : [Constituent Email Addresses](https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons_email.csv)\n",
    "3. `df_subs` : [Constituent Subscription Status](https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons_email_chapter_subscription.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "This section details the data sources for the different output files generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People file data source\n",
    "\n",
    "This section details which dataframe is used to gather the data output in the persons file:\n",
    "\n",
    "| Column | Type | Description | Dataframe source | \n",
    "| :-- | :-- | :-- | :-- |\n",
    "|email | string | Primary email address | `df_emails.email` |\n",
    "|code | string | Source code | `df_info.source` |\n",
    "|is_unsub | boolean | If primary email address is unsubscribed | `df_subs.is_unsub` |\n",
    "|created_dt | datetime | Person creation datetime | `df_info.create_dt` |\n",
    "|updated_dt | datetime | Person updated datetime | `df_info.modified_dt` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition facts data source\n",
    "Data source for the acquisition data file\n",
    "\n",
    "| Column | Type | Description | Data source |\n",
    "| :-- | :-- | :-- | :-- |\n",
    "| acquisition_date | date | Calendar date of acquisition | Date extracted from `created_dt` from people file | \n",
    "| acquisitions | int | Number of constituents acquired on acquisition date | Count() extracted from number of emails on `created_dt` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep relevant columns\n",
    "To increase notebook performance and readability, I only keep columns that are relevant to the analysis.\n",
    "\n",
    "Based on the questions outlined in this exercise and the columns identified in [Identify relevant information & entity relationships](#identify-relevant-information-&-entity-relationships), I am interested in the following columns for each dataframe:\n",
    "\n",
    "\n",
    "**df_info:**\n",
    "- `cons_id`: primary key, relates to other columns\n",
    "- `source`: code string data for q1 table\n",
    "- `create_dt`: created_dt datetime data for q1 table\n",
    "- `modified_dt`: updated_dt datetime for q1 table\n",
    "\n",
    "**df_emails:**\n",
    "- `cons_email_id`: primary key\n",
    "- `cons_id`: foreign key used to link to df_info\n",
    "- `email`: email string data for q1 table\n",
    "\n",
    "**df_subs:**\n",
    "- `cons_email_chapter_subscription_id`: primary key\n",
    "- `cons_email_id`: foreign key to link to df_emails\n",
    "- `isunsub`: is_unsub boolean data for q1 table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational joins\n",
    "The three data files contain relational data with contain primary and foreign keys to connect information about users. Relational joins are required to retrieve the information for the people file.\n",
    "\n",
    "I use `df_emails` as the base data file because it contains the most user records.\n",
    "Left joins to the df_emails table are made to preserve email addresses. This is based on the assumption that email addresses are the variable of interest.\n",
    "\n",
    "Given df_emails, the following left joins can be made:\n",
    "- `df_emails.cons_email_id` = `df_subs.cons_email_id`\n",
    "- `df_emails.cons_id` = `df_info.cons_id`\n",
    "\n",
    "*NOTE*: Because I used a left_join, it's possible that there will be NULL values in merged columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data\n",
    "The script handles missing data based on the intent of the data. \n",
    "\n",
    "Questions 1 and 2 are interested in the following data:\n",
    "1. `email`\n",
    "1. `code`\n",
    "1. `is_unsub`\n",
    "1. `created_dt`\n",
    "1. `updated_dt`\n",
    "\n",
    "`email` is the unique identifier for each row and used to count constituents 'acquired'. Additionally, I use the variable `create_dt` from the constituent information file to record acquisition dates. Because `email` is a primary key and `create_dt` is used to create the acquisition_facts.csv, I will drop records with NULL values in these fields.\n",
    "\n",
    "The folloiwng data: `code`, `is_unsub`, and `updated_dt` are descriptive information about the constituent. This data is not necessary for the acquisition_facts output for question 2. As such, missing data in these columns will be filled witht he string \"unknown\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate values\n",
    "Because `email` is the primary key used in the csv outputs, I remove all duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate foreign keys\n",
    "In exploratory analysis, I discovered that there are duplicate foriegn keys in `df_emails` and `df_subs`.\n",
    "\n",
    "### df_emails.cons_id\n",
    "First, there are duplicate foreign keys in `df_emails.cons_id`. It *is* a possibility that `df_emails.cons_id` to `df_info.cons_id` is a one to many relationship, although unlikely due to there being two datetimes values in each row. \n",
    "\n",
    "**Note**: Because analysis shows that 86% of `df_emails` contains duplicate foreign keys, I opt to keep the the duplicates of `df_emails.cons_id` rather than lose valuable data. **This decision will affect the acquisition_facts.csv output**. In the case that the duplicate `df_emails.cons_id` foreign keys _should_ be removed, I left a commented function to remove the duplicate keys.\n",
    "\n",
    "\n",
    "### df_subs.cons_email_ids\n",
    "Additionally, there are duplicate foreign keys in `df_subs.cons_email_ids`. For this analysis, I remove the rows with duplicate `cons_email_ids` foreign keys. I remove the duplicate keys to avoid erroneously providing incorrect information on the `inunsub` status of Constituents. Sending emails to unsubscribed Constituents may run into legal issues under the CAN-SPAM Act. \n",
    "\n",
    "*Note*: further analysis may explore if rows with duplicate `cons_email_id` contain identical information (i.e., `isunsub`). If so, analysis can keep one instance of each duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##########\n",
      "# Loaded libraries\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "## Uncomment below code to install non-standard 3rd party libraries\n",
    "# missingno used to identify missing data\n",
    "# pywrangle is an open-source library I am developing for data cleaning\n",
    "# ! pip install missingno\n",
    "# ! pip install pywrangle  \n",
    "\n",
    "\n",
    "import missingno as msno\n",
    "import pywrangle as pw\n",
    "\n",
    "header = '#' * 10\n",
    "print('\\n', header, \"# Loaded libraries\", header, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 3 data sources are used:\n",
    "\n",
    "1. [Constituent Information](https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons.csv)\n",
    "2. [Constituent Email Addresses](https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons_email.csv)\n",
    "3. [Constituent Subscription Status](https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons_email_chapter_subscription.csv)\n",
    "\n",
    "*Note*: Boolean columns (including is_primary) in all of these datasets are 1/0 numeric values. 1 means True, 0 means False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Urls to download csvs\n",
    "url_info_csv = 'https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons.csv'\n",
    "url_email_csv = 'https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons_email.csv'\n",
    "url_sub_csv = 'https://als-hiring.s3.amazonaws.com/fake_data/2020-07-01_17%3A11%3A00/cons_email_chapter_subscription.csv'\n",
    "\n",
    "url_csvs = (\n",
    "    url_info_csv,\n",
    "    url_email_csv,\n",
    "    url_sub_csv\n",
    ")\n",
    "\n",
    "## Csv file names\n",
    "csv_datafiles = cons_data_csvs = (\n",
    "    'cons.csv',      ## info\n",
    "    'cons_email.csv',                      \n",
    "    'cons_email_chapter_subscription.csv'   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename_csv: str, url_csv: str) -> object:\n",
    "    \"\"\"Auxiliary function to load csvs into dataframe.\n",
    "    \n",
    "    Checks if csv exists in working directory. \n",
    "    If not, downloads csv from url.\n",
    "    \"\"\"\n",
    "    file_path = f'./{filename_csv}'\n",
    "    \n",
    "    print('\\n', '#' * 5, filename_csv)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(\"Reading from working directory.\")\n",
    "        df = pd.read_csv(filename_csv)\n",
    "    \n",
    "    else:\n",
    "        print(\"Loading data from url. This is a large file, so please be patient.\")\n",
    "        df = pd.read_csv(url_csv)\n",
    "        \n",
    "        print(f\"\\tSaving {filename_csv} to working directory.\")\n",
    "        df.to_csv(path_or_buf = filename_csv)\n",
    "    \n",
    "    print(f\"\"\"Loaded df for {filename_csv}:\n",
    "    - {len(df.columns)} columns\n",
    "    - {len(df)} rows\"\"\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ##### cons.csv\n",
      "Loading data from url. This is a large file, so please be patient.\n",
      "\tSaving cons.csv to working directory.\n",
      "Loaded df for cons.csv:\n",
      "    - 29 columns\n",
      "    - 700000 rows\n",
      "\n",
      " ##### cons_email.csv\n",
      "Loading data from url. This is a large file, so please be patient.\n"
     ]
    }
   ],
   "source": [
    "## Load all csvs\n",
    "\n",
    "df_info, df_emails, df_subs = (load_df(csv_datafiles[i], url_csvs[i]) for i in range(len(csv_datafiles)))\n",
    "all_cons_dfs = (df_info, df_emails, df_subs)\n",
    "\n",
    "print(\"\\nLoaded all csv data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face check data\n",
    "This section does a face check of the data to look at the contents of each dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify relevant information & entity relationships\n",
    "This section looks at the schema of each dataframe, identifies relevant columns for the ETL exercise, and primary/foreign keys to connect data frame results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant columns in **df_info**:\n",
    "- `cons_id`: primary key, relates to other columns\n",
    "- `source`: code string data for q1 table\n",
    "- `create_dt`: created_dt datetime data for q1 table\n",
    "- `modified_dt`: updated_dt datetime for q1 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant columns in **df_emails**:\n",
    "- `cons_email_id`: primary key\n",
    "- `cons_id`: foreign key used to link to df_info\n",
    "- `email`: email string data for q1 table\n",
    "\n",
    "*NOTE:*\n",
    "There are two unclear elements in this table.\n",
    "- `create_dt`: unclear if this may also fulfill q1 table requirement.\n",
    "- `modified_dt`: unclear if this may also fulfill q1 table requirement\n",
    "\n",
    "However, because these are associated in the email information, I will assume that the `create_dt` and `modified_dt` is used for the email, and not the person. As such, I will use the `create_dt` and `modified_dt` in df_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant columns in **df_subs**:\n",
    "- `cons_email_chapter_subscription_id`: primary key\n",
    "- `cons_email_id`: foreign key to link to df_emails\n",
    "- `isunsub`: is_unsub boolean data for q1 table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity relationship notes\n",
    "The three data files contain relational data with contain primary and foreign keys to connect information about users. Relational joins are required to retrieve the information for the people file.\n",
    "\n",
    "I use `df_emails` as the base data file because it contains the most user records.\n",
    "Left joins to the df_emails table are made to preserve email addresses. This is based on the assumption that email addresses are the variable of interest.\n",
    "\n",
    "Given df_emails, the following left joins can be made:\n",
    "- `df_emails.cons_email_id` = `df_subs.cons_email_id`\n",
    "- `df_emails.cons_id` = `df_info.cons_id`\n",
    "\n",
    "*NOTE*: Because I used a left_join, it's possible that there will be NULL values in merged columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep relevant columns\n",
    "To increase notebook performance and readability, I only keep columns that are relevant to the analysis.\n",
    "\n",
    "Based on the questions outlined in this exercise and the columns identified in [Identify relevant information & entity relationships](#identify-relevant-information-&-entity-relationships), I am interested in the following columns for each dataframe:\n",
    "\n",
    "\n",
    "**df_info:**\n",
    "- `cons_id`: primary key, relates to other columns\n",
    "- `source`: code string data for q1 table\n",
    "- `create_dt`: created_dt datetime data for q1 table\n",
    "- `modified_dt`: updated_dt datetime for q1 table\n",
    "\n",
    "**df_emails:**\n",
    "- `cons_email_id`: primary key\n",
    "- `cons_id`: foreign key used to link to df_info\n",
    "- `email`: email string data for q1 table\n",
    "\n",
    "**df_subs:**\n",
    "- `cons_email_chapter_subscription_id`: primary key\n",
    "- `cons_email_id`: foreign key to link to df_emails\n",
    "- `isunsub`: is_unsub boolean data for q1 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## columns to save for each df\n",
    "cols_df_info = [\n",
    "    'cons_id',\n",
    "    'source',\n",
    "    'create_dt',\n",
    "    'modified_dt'\n",
    "]\n",
    "cols_df_email = [\n",
    "    'cons_email_id',\n",
    "    'cons_id',\n",
    "    'email',\n",
    "#     'create_dt',\n",
    "#     'modified_dt',\n",
    "]\n",
    "cols_df_subs = [\n",
    "    'cons_email_chapter_subscription_id',\n",
    "    'cons_email_id',\n",
    "    'isunsub'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxilary function to modify df and track changes\n",
    "def keep_df_cols(df: object, columns: list) -> object:\n",
    "    \"\"\"Returns pandas dataframe with only specified columns.\n",
    "    \n",
    "    Uses pywrangle to document df changes.\"\"\"\n",
    "    df_old = pw.record_df_info(df)\n",
    "    df = df[columns]\n",
    "    pw.print_df_changes(df, df_old)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Keep information columns\n",
    "df_info = keep_df_cols(df_info, cols_df_info)\n",
    "df_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 25 columns from `df_info` and 86% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Keep email columns\n",
    "df_emails = keep_df_cols(df_emails, cols_df_email)\n",
    "df_emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 13 columns from `df_emails` and 81% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Keep subscription columns\n",
    "df_subs = keep_df_cols(df_subs, cols_df_subs)\n",
    "df_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 3 columns from `df_subs` and 50% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "Look at the missing data in each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df_missing_data(df: object, df_name: str) -> None:\n",
    "    \"\"\"Creates msno.bar graph of data present in dataframe.\"\"\"\n",
    "    msno.bar(df)\n",
    "    plt.title(f\"Non-NULL values for {df_name} data.\")\n",
    "    plt.xlabel(\"Column\")\n",
    "    plt.ylabel(\"% data present\")\n",
    "    plt.figure(figsize=(5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing constituent info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_missing_data(df_info, \"Constituent Information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: There `source` column is missing approximately half of its data. Because there is still relevant information on for `create_dt` and `modified_dt`, I will keep missing data in the source columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source data is categorical with 4 categories:\n",
    "- organic\n",
    "- google\n",
    "- twitter\n",
    "- facebook\n",
    "\n",
    "Because about half of source values are NULL, I will preserve them and label them as `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info['source'].fillna(\"unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NULLs are transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_df_missing_data(df_info, \"Constituent Information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing email data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_missing_data(df_emails, \"Constituent Emails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in `df_emails`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing subscription data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_missing_data(df_subs, \"Constituent Subscription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in `df_subs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean string data\n",
    "This section cleans string data in the data frame columns.\n",
    "\n",
    "All strings are lower cased in the column for consistency purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize constituent information\n",
    "Use pywrangle to standardize string column casing. Pass a tuple of tuples containing a string indicating the column to clean, and an integer representing the cleaning method to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_info_strcol_caseint: Tuple[Tuple[ str, int]] = (\n",
    "    (\"source\", 0),\n",
    "    (\"create_dt\", 0),\n",
    "    (\"modified_dt\", 0)\n",
    ")\n",
    "df_info = pw.clean_str_columns( df_info, cons_info_strcol_caseint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_email_strcol_caseint: Tuple[Tuple[ str, int]] = (\n",
    "    ('email', 0),\n",
    ")\n",
    "df_emails = pw.clean_str_columns( df_emails, cons_email_strcol_caseint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize subscription information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No string data in `df_subs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicates\n",
    "This section checks for duplicate data entries in the Constituent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliary function to check duplicates\n",
    "def check_df_duplicates(df, check_columns: List[str]) -> None:\n",
    "    \"\"\"Prints df and df.size of duplicates rows, and duplicates for specified columns.\"\"\"\n",
    "    ## duplicate rows\n",
    "    df_duplicate_rows = df[df.duplicated() == True]\n",
    "    print(f\"Number of df duplicate rows {len(df_duplicate_rows)}\")\n",
    "    \n",
    "    ## duplicates in column\n",
    "    for col in check_columns:\n",
    "        df_col_duplicates = df[df.duplicated(col)]\n",
    "        print(f\"Number of duplicates in {col}: {len(df_col_duplicates)}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_percent_missing_data(df, df_dup) -> None:\n",
    "    \"\"\"Prints the percentage of missing data.\"\"\"\n",
    "    print( (df_dup.size) / df.size * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliary function to remove duplicates\n",
    "def remove_df_duplicates(df, column: str, keep: Union[str, bool] = False) -> \"dataframe\":\n",
    "    \"\"\"Auxiliary method to remove rows with duplicates n specified column.\n",
    "    \n",
    "    Uses pywrangle to record difference in df.\"\"\"\n",
    "    old_df = pw.record_df_info(df)\n",
    "    df.drop_duplicates(subset = column, keep = keep, inplace = True)\n",
    "    pw.print_df_changes(df, old_df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_info.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_info_cols = ['cons_id']   # no duplicate primary keys.\n",
    "check_df_duplicates(df_info, check_columns = check_info_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### email duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_emails.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_email_cols = ['cons_email_id', 'cons_id']   # No duplicate primary or foreign keys\n",
    "check_df_duplicates(df_emails, check_columns = check_email_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup_col = 'cons_id'\n",
    "df_dup_emails = df_emails[df_emails.duplicated(dup_col, keep = False)].sort_values(dup_col)\n",
    "df_dup_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.sort_values(dup_col).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of duplicate foreign keys for cons_id and we cannot be sure which user should reference the cons_id.\n",
    "\n",
    "It *is* a possibility that `df_emails.cons_id` to `df_info.cons_id` is a one to many relationship, although unlikely due to there being two datetimes values in each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_percent_missing_data(df_emails, df_dup_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: Because analysis shows that 86% of the data contains duplicate foreign keys, I opt to keep the the duplicates of `df_emails.cons_id` rather than losing valuable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_emails = remove_df_duplicates(df_emails, dup_col, keep = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subscription duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_subs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sub_cols = ['cons_email_chapter_subscription_id', 'cons_email_id']  # primary and foreign keys\n",
    "check_df_duplicates(df_subs, check_columns= check_sub_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_col = 'cons_email_id'\n",
    "df_dup_subs = df_subs[ df_subs.duplicated(dup_col, keep = False)].sort_values(dup_col)\n",
    "df_dup_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are duplicate foreign keys in `df_subs.cons_email_ids`. For this analysis, I remove the rows with duplicate `cons_email_ids` foreign keys. I remove the duplicate keys to avoid erroneously providing incorrect information on the `inunsub` status of Constituents. Sending emails to unsubscribed Constituents may run into legal issues under the CAN-SPAM Act.\n",
    "\n",
    "Note: further analysis may explore if rows with duplicate `cons_email_id` contain identical information (i.e., `isunsub`). If so, analysis can keep one instance of each duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_percent_missing_data(df_subs, df_dup_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subs = remove_df_duplicates(df_subs, dup_col, keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sanity check that the rows from `df_subs` == `len(df_dup_subs)`\n",
    "len(df_dup_subs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean datettime\n",
    "datetime data stored in df_info is stored in the {Day, Year-Month-Day Hour:Minute:Second} format. \n",
    "I convert this into ISO-datetime format for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliary function to get datetime from day, datetime information stored in\n",
    "def get_datetime(day_datetime: str) -> 'datetime':\n",
    "    \"\"\"Returns datetime object from str datetime.\n",
    "    \n",
    "    >>> get_datetime('sat, 2017-09-30 08:26:54') \n",
    "    2017-09-30 08:26:54\n",
    "    \"\"\"\n",
    "    int_index = re.search(r'\\d', day_datetime).start()\n",
    "    datetime_list = list(day_datetime)[int_index:]\n",
    "    return ''.join(datetime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify date-time format parser\n",
    "datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "## Cols to iterate through\n",
    "datetime_cols = ('create_dt', 'modified_dt')\n",
    "\n",
    "for col in datetime_cols:\n",
    "    df_info[col] = df_info[col].map(get_datetime)\n",
    "    df_info[col] = pd.to_datetime(df_info[col], format = datetime_format)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_dt` and `modified_dt` are now registered as datetime64 objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join data\n",
    "In this section, I join the three dataframes into a single dataframe with the desired information for quesiton 1:\n",
    "- email\n",
    "- code\n",
    "- is_unsub\n",
    "- created_dt\n",
    "- updated_dt\n",
    "\n",
    "Given df_emails, the following left joins are made:\n",
    "- df_emails.cons_email_id = df_subs.cons_email_id\n",
    "- df_emails.cons_id = df_info.cons_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Left Join df_emails and df_info\n",
    "df_email_info = pd.merge(left = df_emails, right = df_info, how= 'left', left_on = 'cons_id', right_on = 'cons_id')\n",
    "df_email_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create total data_frame for q1\n",
    "df_email_info_subs = pd.merge(\n",
    "    left = df_email_info, right = df_subs, \n",
    "    how = 'left', \n",
    "    left_on = 'cons_email_id', right_on = 'cons_email_id')\n",
    "df_email_info_subs.sort_values(by = 'cons_id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email_info_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: 'People' File\n",
    "This section saves the 'people' file as a csv with a header line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns = [\n",
    "    'email',\n",
    "    'source',\n",
    "    'isunsub',\n",
    "    'create_dt',\n",
    "    'modified_dt'\n",
    "]\n",
    "df_q1 = df_email_info_subs[desired_columns]\n",
    "df_q1.reset_index(drop = True, inplace = True)\n",
    "df_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "q1_col_names = (\n",
    "    'email',\n",
    "    'code',\n",
    "    'is_unsub',\n",
    "    'created_dt',\n",
    "    'updated_dt'\n",
    ")\n",
    "df_q1.columns = q1_col_names\n",
    "df_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_missing_data(df_q1, \"Dataframe for q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The left join on emails to subcription status created NULL values in the `in_unsub` column.\n",
    "\n",
    "Because the second question does not specify to remove people who are unsubbed, I will preserve rows with NULL values in the `in_unsub` column. I will mark these values as `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = 'is_unsub'\n",
    "df_q1[missing_col].fillna(\"unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_missing_data(df_q1, \"Dataframe for q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'people.csv'\n",
    "\n",
    "df_q1.to_csv(\n",
    "    path_or_buf = file_name,\n",
    "    index = False,)\n",
    "\n",
    "print(f\"Wrote dataframe to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: “Acquisition_facts” file with stats about when people where acquired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract date from created_dt\n",
    "`created_dt` is stored as datetime. This analysis only wants the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1['acquired'] = df_q1['created_dt'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group values by date\n",
    "This section creates a dataframe with count values for each acquired date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_var = ['acquired']\n",
    "\n",
    "df_q2 = (\n",
    "    df_q1\n",
    "    .groupby( group_var, as_index = True)\n",
    "    .count()\n",
    "    .sort_values(group_var, ascending = True)['email']\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "df_q2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: date starts at Epoch time - likely reflects the fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(df_q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"acquisition_facts.csv\"\n",
    "\n",
    "df_q2.to_csv(\n",
    "    path_or_buf = file_name,\n",
    "    index = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
